#!/usr/bin/env python
# coding: utf-8

# In[37]:


# In[98]:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as mno
import requests
from sklearn.model_selection import train_test_split


import psycopg2
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT
import os
from dotenv import load_dotenv

# Charger les variables du fichier .env
load_dotenv()

DB_CONFIG = {
    "dbname": os.getenv("DB_NAME"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "host": os.getenv("DB_HOST"),
    "port": os.getenv("DB_PORT"),
}

print('Ma config db : ',DB_CONFIG)




# # Import du data set  

# In[39]:


df = pd.read_csv('/Users/laura.b/Documents/EPSI/project/mspr-1-etl/data/worldometer_data_raw.csv')


# # Analyse du dataset

# In[40]:


df.head()


# Premi√®rement, on d√©finit/d√©crit nos variables:
# 
# - *Country/Region* : Liste des pays affect√©s par la COVID-19
# 
# - *Total Cases* : Nombre cumulatif de cas confirm√©s √† ce jour
# 
# - *New Cases* : Nombre de nouveaux cas confirm√©s chaque jour
# (Contexte temporel manquant!)
# 
# - *Total Deaths* : Nombre cumulatif de d√©c√®s √† ce jour
# 
# - *New Deaths* : Nombre de nouveaux d√©c√®s chaque jour
# (Contexte temporel manquant!)
# 
# - *Total Recovered* : Nombre cumulatif de cas r√©tablis √† ce jour
# 
# - *Active Cases* : Nombre cumulatif de cas actifs √† ce jour
# (Remarque : la d√©finition d‚Äôorigine indiquait ¬´ cas r√©cup√©r√©s ¬ª, mais il s‚Äôagit vraisemblablement d‚Äôune erreur. Il est attendu que cette variable repr√©sente les cas encore actifs.)
# 
# - *Serious, Critical* : Nombre cumulatif de cas graves/critique √† ce jour
# 
# - *TotalCases/1M pop* : Nombre cumulatif de cas confirm√©s √† ce jour pour un million d‚Äôhabitants
# 
# - *Deaths/1M pop* : Nombre cumulatif de d√©c√®s √† ce jour pour un million d‚Äôhabitants
# 
# - *Total Tests* : Nombre cumulatif de tests effectu√©s √† ce jour
# (Contexte temporel manquant!)
# - *Tests/1M pop* : Nombre cumulatif de tests effectu√©s √† ce jour pour un million d‚Äôhabitants
# (Contexte temporel manquant!)
# 
# - *WHO Population*: L'Organisation mondiale de la sant√© divise le monde en six r√©gions.

# In[41]:


df.info()


# 
# 
# La m√©thode **df.info()** nous permet d'avoir une vue d'ensemble sur la structure du dataset : 
# 
# **structure du dataset** : 
# 
# - "shape" : 209 entr√©es (lignes) et 16 variables(colonnnes).
# - Le typage des donn√©es et le nombre de variables par type de donn√©es  : on a des float64, int64 et object 
# - Le nombre de valeurs non-nulles (donc les nulles) 
# 
# Ici on voit que toutes les variables comportent des valeurs nulles, √† l'exception de "Country/Region" et "TotalCases" √©tant qu'ils ont 209 valeures non nulles 
# 
# On remarque √©galement que certaines variables ont les m√™mes nombre de valeurs manquantes. Ces variables/colonnes ne sont-elles pas des doublons?
# 
# L'objectif est de v√©rifier les donn√©es du dataset et de savoir si celles fournient dans le dataset est fiable 

# ### Etape 0 : R√©agencement des colonnes 
# D'abord nous allons r√©agencer l'ordre des colonnes pour une meilleures lisibilit√©. 
# Elles ont √©t√© r√©organis√©es en commen√ßant par les variables cat√©gorielles d'un cot√© et les num√©riques de l'autre pour une meilleure lisibilit√© des colonnes.
# 

# In[42]:


df.columns


# In[43]:


df = df[['Continent', 'WHO Region','Country/Region', 'Population', 'TotalCases', 'NewCases','TotalDeaths', 'NewDeaths', 'TotalRecovered', 'NewRecovered','ActiveCases', 'Serious,Critical', 'Tot Cases/1M pop', 'Deaths/1M pop','TotalTests', 'Tests/1M pop']]


# In[44]:


mno.matrix(df, figsize=(15,5))


# ## Etape 1 : Matplot 
# ### La heatmap 
# La heatmap est une repr√©sentation visuelle permettant d'observer s'il y a des corr√©lations entre les colonnes du dataset

# In[45]:


plt.figure(figsize=(10, 7))
sns.heatmap(df.select_dtypes('number').corr(), annot=True, fmt=".1f", cmap="coolwarm")


# #### R√©sultat d'analyse de la heatmap
# *Explication des couleurs de matrice* : 
# - Les valeurs proches de *1.0* (en rouge fonc√©) indiquent une forte corr√©lation positive 
# - Les valeurs proches de *-1.0* (en bleu fonc√©) indiquent une forte corr√©lation n√©gative
# - Les valeurs proches de *0* (en couleurs p√¢les) indiquent peu ou pas de corr√©lation
# 
# On peut voir une large zone rouge fonc√© dans le haut √† gauche de la heatmap ce qui signifie qu'il y a un grand nombre de valeurs √† forte cor√©√©lation. 
# 
# Om compte parmi elles, les colonnes (TotalCases, NewCases, TotalDeaths, NewDeaths, TotalRecovered, NewRecovered, ActiveCases, Serious,Critical).
# Il est logique qu'on est une forte corr√©lation entre TotalCases et TotalDeaths 
# TotalCases est fortement corr√©l√© √† (0.9) avec TotalCases et TotalDeaths ce qui est logique puisque les morts font partis des donn√©es de Total Cases et TotalDeaths.

# ### Boxplot

# In[46]:


plt.figure(figsize=(19,9))
sns.boxplot(data=df,palette='Accent')
plt.title("Donn√©es Statistiques du dataset d'origine sans nettoyage")
plt.show()


# In[ ]:





# Le *boxplot* qui repr√©sente l'int√©gralit√© du dataset ne peut pas √™tre analys√© correctement car il contient des valeurs extr√®mes qui emp√¢chent la lisibilit√© des autres donn√©es. Nous allons isol√© chaque colonne dans un boxplot afin d'avoir une mise √† l'√©cehelle ad√©quate avec les valeurs concern√©es. 

# In[47]:


df.boxplot(column=['Population'],
figsize=(9,5))
plt.title("Donn√©es Statistiques de la colonne Population")
plt.show()


# 

# In[48]:


df.boxplot(column=['NewCases','NewDeaths','NewRecovered'], figsize=(9,5))
plt.title("Donn√©es Statistiques sur les nouveax morts, cas et sauv√©es")
plt.show()


# In[49]:


df.boxplot(column=['TotalDeaths'], figsize=(9,5))
plt.title("Donn√©es Statistiques de la colonne TotalDeaths")
plt.show()


# In[50]:


df.boxplot(['TotalRecovered', 'ActiveCases', 'TotalCases','Tests/1M pop'], figsize=(9,5))
plt.title("Donn√©es Statistiques de la colonne Population")
plt.show()


# In[51]:


df.boxplot(column=['TotalTests'], figsize=(9,5))
plt.title("Donn√©es Statistiques de la colonne TotalDeaths")
plt.show()


# In[52]:


df.boxplot(column=['Tests/1M pop'], figsize=(9,5))
plt.title("Donn√©es Statistiques de la colonne TotalDeaths")
plt.show()


# In[53]:


df.boxplot(column=['Serious,Critical'], figsize=(9,5))
plt.title("Donn√©es Statistiques de la colonne TotalDeaths")
plt.show()


# In[54]:


df.boxplot(column=['Deaths/1M pop'], figsize=(9,5))
plt.title("Donn√©es Statistiques sur le nombre de morts/ 1M de population")
plt.show()


# In[55]:


df.boxplot(column=['Tot Cases/1M pop'], figsize=(9,5))
plt.title("Donn√©es Statistiques sur ")
plt.show()


# In[56]:


df.boxplot(column=['Tests/1M pop'], figsize=(9,5))
plt.title("Donn√©es Statistiques de la colonne Population")
plt.show()


# 

# In[57]:


df.describe()


# Maintenant, on fait une somme des valeurs manquantes:

# In[58]:


df.isna().sum()


# In[59]:


mno.matrix(df, figsize=(15,5))


# In[60]:


df.isna().sum()


# ## Analyse des valeures nulles par variables

# In[61]:


mno.matrix(df, figsize=(15,5))


# On peut voir dans la colonne "Continent" et "Population qu'il y a une valeure "null". On peut supposer qu'un pays n'aurait pas √©t√© affili√© √† un continent puisque cette abscence de donn√©es est situ√© sur la m√™me ligne du dataset comme on peut le voir la matrix ci-dessus. 

# In[62]:


mask_population = df['Population'].isnull() | (df['Population'] == '')
mask_continent = df['Continent'].isnull() | (df['Continent'] == '')

ligne_suspecte = df[mask_population & mask_continent]

print("Ligne suspecte avec les colonnes Population et Continent vides:")
ligne_suspecte


# Apr√®s avoir mener des recherches . Le 'Diamond Princess' est un bateau et non un pays. On peut expliquer sa pr√©sence sur la dataset car le paquebot a √©t√© mis en quarantaine √† cause de cas covid d√©tect√© √† bord. C'est donc pas un pays donc on va le retirer du dataset.

# # Nettoyage des donn√©es

# In[63]:


df = df[df['Country/Region'] != 'Diamond Princess']


# In[64]:


mno.matrix(df, figsize=(15,5))


# On peut voir en se r√©f√©rant √† la matrice que les colonnes "Continent" et "Population" ne pr√©sentent plus de valeurs manquantes visuellement, confirmant que les donn√©es nulles ont √©t√© correctement trait√©es.

# In[65]:


df.isna().sum()


# On supprime les doublons en utilisant la m√©thode **drop_duplicates()**

# In[66]:


df.drop_duplicates(inplace = True)


# In[67]:


df.duplicated()


# On constate qu'il n'y a pas de doublons dans le dataset comme le nombre de lignes est identique √† celui avant d'appliquer la m√©thode.

# In[ ]:





# In[68]:


# Cr√©er une colonne 
df['NewTotalCases'] = df['TotalDeaths'] + df['TotalRecovered'] + df['ActiveCases']
df


# In[69]:


plt.figure(figsize=(10, 7))
sns.heatmap(df.select_dtypes('number').corr(), annot=True, fmt=".1f", cmap="coolwarm")


# Cette figure permet de mieux repr√©senter les valeurs nulles du dataset.
# 
# Mais aussi de se rendre compte de certaines aberrations.
# 
# Ici, on a des colonnes qui sont les copies conformes, on peut supposer que ce sont des "variables doublons":
# 
# - TotalCases et Tot Cases/1M pop ??
# - TotalDeaths et Deaths/1M pop ?
# - NewDeaths et NewRecovered ?
# - TotalRecovered et ActiveCases ?
# - TotalTests et Tests/1m pop ?
# 
# 

# Nous allons donc supprimer ces derni√®res √©tant donn√©e qu'elles ont un niveau de corr√©lation √† 1

# In[70]:


df = df.drop(columns=["Tot Cases/1M pop", "Deaths/1M pop", "NewRecovered", "NewCases","Tests/1M pop","NewDeaths"], errors='ignore')


# In[71]:


df.head()


# Ces deux points sont importants pour la pr√©paration des donn√©es :
# 
# - **Suppression des variable**s d√©riv√©es : Les m√©triques normalis√©es "Tot Cases/1M pop", "Deaths/1M pop" et "Tests/1M pop" sont calculables √† partir des donn√©es brutes existantes (TotalCases/Population, TotalDeaths/Population, etc.). Si on les conservait cela cr√©erait une redondance qui pourrait biaiser les analyses.
#  
# - **Suppression des colonnes** avec trop de valeurs manquantes : "NewRecovered", "NewDeaths" et "NewCases" pr√©sentent plus de 200 valeurs manquantes, ce qui repr√©sente une proportion significative du dataset. Au lieu, d'imputer ces nombreuses valeurs manquantes, la suppression de ces colonnes est une approche raisonnable pour maintenir la qualit√© des donn√©es.

# In[72]:


mno.matrix(df, figsize=(15,5))


# In[73]:


plt.figure(figsize=(10, 7))
sns.heatmap(df.select_dtypes('number').corr(), annot=True, fmt=".2f", cmap="coolwarm")


# La corr√©lation est forte entre les colonnes TotalCases et TotalDeaths car les donn√©es de TotalDeaths sont contenus dans TotalCases.
# On peut faire le m√™me constat pour les variables TotalRecovered et TotalTests. 

# In[74]:


mno.matrix(df, figsize=(15,9))


# On remplaceme les **NaN** par "**Non class√©"** pour la colonne WHO Region.

# In[75]:


df['WHO Region'] = df['WHO Region'].fillna("Non class√©")


# In[103]:


#Afficher les donn√©es du data set en aleatoire 
df.sample(20)


# On s'interroge sur comment on va remplacer les valeurs num√©riques manquantes par z√©ro, par la m√©diane, la moyenne. 
# Quelle est la meilleure option pour avoir des donn√©es coh√©rentes ?
# 
# On proc√®de au remplacement des valeurs nulles par des formules logiques.
# 
# **Pourquoi transformer TotalCases, TotalDeaths et TotalRecovered ?** <br>
# 
# Ces transformations aident √† r√©duire la variance excessive dans les donn√©es. Sans cela, la variance serait domin√©e par quelques grands pays avec des chiffres tr√®s √©lev√©s, ce qui masquerait les diff√©rences entre la majorit√© des pays. 
# 
# En r√©duisant la variance, nous obtenons une distribution plus √©quilibr√©e o√π tous les pays peuvent √™tre compar√©s de mani√®re plus √©quitable.

# ### Imputation des valeurs manquantes de la colonne "ActiveCases":

# In[76]:


# Afficher les lignes o√π ActiveCases est null
null_active_cases = df[df['ActiveCases'].isnull()]
null_active_cases


# In[5221]:


# Filtrer le DataFrame pour ne conserver que les pays d'Europe
europe_df = df[df['Continent'] == 'Europe']

# Calculer la m√©diane de ActiveCases pour l'Europe
europe_active_cases_median = europe_df['ActiveCases'].median()

# Remplacer les valeurs NaN dans le DataFrame original
df['ActiveCases'] = df['ActiveCases'].fillna(europe_active_cases_median)

# V√©rifier le nombre de valeurs NaN apr√®s remplacement
nan_count_after = df['ActiveCases'].isnull().sum()

# Cr√©er un nouveau DataFrame avec seulement les lignes sp√©cifi√©es
selected_rows = df.iloc[[9, 11, 29, 40]]
selected_rows


# ### Imputation des valeurs manquantes de la colonne "TotalDeaths":

# In[5222]:


#Remplace les donn√©es manquantes de la colonne "TotalDeaths" par le r√©sultat de la formule : "TotalCases" - "TotalRecovered" + "ActiveCases"

def fill_missing_total_deaths(row):
    if pd.isnull(row['TotalDeaths']):
        return int(round(row['TotalCases'] - (row['TotalRecovered'] + row['ActiveCases'] )))
    return row['TotalDeaths']

missing_total_deaths_before = df['TotalDeaths'].isnull().sum()

df['TotalDeaths'] = df.apply(fill_missing_total_deaths, axis=1)

missing_total_deaths_after = df['TotalDeaths'].isnull().sum()

print(f"Number of missing 'TotalDeaths' values before filling: {missing_total_deaths_before}")
print(f"Number of missing 'TotalDeaths' values after filling: {missing_total_deaths_after}")


# ### Imputation des valeurs manquantes de la colonne "TotalRecovered":

# In[81]:


#Remplacer les donn√©es manquantes de la colonne "TotalRecovered" par le r√©sultat de la formule : "TotalCases" - "TotalDeaths" + "ActiveCases"

def fill_missing_total_recovered(row):
    if pd.isnull(row['TotalRecovered']):
        return int(round(row['TotalCases'] - (row['TotalDeaths'] + row['ActiveCases'])))
    return row['TotalRecovered']

missing_total_deaths_before = df['TotalRecovered'].isnull().sum()

df['TotalRecovered'] = df.apply(fill_missing_total_recovered, axis=1)

missing_total_deaths_after = df['TotalRecovered'].isnull().sum()

selected_rows = df.iloc[[9, 11, 29, 40]]
selected_rows


# In[82]:


# Cr√©er une colonne calcul√©e
df['NewTotalCases'] = df['TotalDeaths'] + df['TotalRecovered'] + df['ActiveCases']
df


# In[83]:


mno.matrix(df, figsize=(15,5))


# In[84]:


# Comparer si deux colonnes sont identiques
if df['NewTotalCases'].equals(df['TotalCases']):
    print("Les colonnes sont identiques")
else:
    print("Les colonnes sont diff√©rentes")
    
    # Afficher le nombre de diff√©rences
    differences_count = (df['NewTotalCases'] != df['TotalCases']).sum()
    print(f"Nombre de diff√©rences trouv√©es : {differences_count}")
    
    # Afficher les premi√®res lignes o√π il y a des diff√©rences
    print("\nExemples de diff√©rences :")
    diff_rows = df[df['NewTotalCases'] != df['TotalCases']][['NewTotalCases', 'TotalCases']]
    print(diff_rows.head())


# In[5227]:


df = df.drop(columns=['TotalCases'])
df


# On peut voir sur la matrice que les colonnes "TotalRecovered" et "TotalDeaths" ne contiennent plus de valeurs nulles 

#  ## Imputation des valeurs manquantes par la m√©diane

# La m√©thode utilis√©e dans ce code remplace les valeurs manquantes de mani√®re plus intelligente :
# 
# D'abord, le code regroupe les pays par continent
# Ensuite, pour chaque continent, il calcule la valeur m√©diane (la valeur du milieu)
# Enfin, il remplace les donn√©es manquantes par la m√©diane du continent correspondant
# 
# Par exemple, si un pays d'Afrique a une valeur manquante pour 'Serious,Critical', cette valeur sera remplac√©e par la m√©diane de tous les pays africains (et non par la m√©diane mondiale).
# L'avantage est que les pays re√ßoivent des valeurs de remplacement qui correspondent mieux √† leur r√©gion g√©ographique, ce qui donne des r√©sultats plus r√©alistes.

#  ### Imputation des valeurs manquantes de la colonne "Serious, Critical":

# In[85]:


# Pour la colonne 'Serious,Critical'
df['Serious,Critical'] = df.groupby('Continent')['Serious,Critical'] \
                             .transform(lambda x: x.fillna(x.median()))

# Pour la colonne 'TotalTests'
df['TotalTests'] = df.groupby('Continent')['TotalTests'] \
                     .transform(lambda x: x.fillna(x.median()))


# 
# 
# **Pourquoi utiliser la m√©diane pour remplacer les valeurs manquantes?**
# 
# La **m√©diane** est plus pertinente que la moyenne dans notre cas car elle n'est pas influenc√©e par les valeurs extr√™mes qui augmentent artificiellement la variance. 
# 
# Par exemple, quelques pays comme la Chine ou les √âtats-Unis avec des chiffres √©normes augmenteraient consid√©rablement la variance et fausseraient la moyenne.
# 
# La m√©diane repr√©sente mieux la situation "typique" de la majorit√© des pays, pr√©servant ainsi une variance plus naturelle dans les donn√©es
# 
# 
# Ces choix m√©thodologiques permettent d'obtenir des donn√©es avec une variance plus repr√©sentative pour r√©aliser une analyse pertinente.

# In[86]:


mno.matrix(df, figsize=(15,5))


# On peut voir sur la matrice que la colonne **"Serious, Critical"** ne contient plus de valeurs nulles 

# In[87]:


plt.figure(figsize=(8, 7))
sns.heatmap(df.select_dtypes('number').corr(), annot=True, fmt=".2f", cmap="coolwarm")


# In[88]:


df.columns


# ## Conversion des valeurs num√©rques en int

# In[90]:


for column in df.columns:
    if df[column].dtype == 'float64':
        df[column] = df[column].fillna(0).astype(int)

df.info()


# # Normalisation des colonnes

# In[92]:


columns_mapping = {
    "Country/Region": "country",
    "WHO Region": "who_region",
    "NewTotalCases": "new_total_cases",
    "TotalDeaths": "total_deaths",
    "TotalRecovered": "total_recovered",
    "Serious,Critical": "serious_critical",
    "TotalTests": "total_tests",
    "Population": "population",
    "Continent": "continent",
    "ActiveCases":"active_cases"
}
df = df.rename(columns=columns_mapping)
df


# # Dataset transform√©

# In[93]:


df = df.drop_duplicates()


# In[94]:


# On affiche le df transform√©
df.head()


# In[95]:


sns.heatmap(df.select_dtypes('number').corr(),annot=True, cmap="coolwarm")


# # Pr√©paration du jeu de donn√©es √† entrainer 

# In[99]:


# Pour un split sans variable cible sp√©cifique avec une r√©partition 80% d'entrainement et 20% de test
X = df  
X_train, X_test = train_test_split(
    X, 
    test_size=0.2,
    random_state=42
)


# Absence de varaibale cible :

# In[5240]:


print(f"Taille de X_train: {X_train.shape}")
print(f"Taille de X_test: {X_test.shape}")


# # Chargement des donn√©es

# ## Connexion √† PostgreSQL

# In[101]:


import psycopg2
import time

def checkpostgres(max_retries=5):
    print("üîç V√©rification de la connexion √† PostgreSQL...", flush=True)
    
    for attempt in range(max_retries):
        print(f"üü° Tentative {attempt + 1}/{max_retries}...", flush=True)

        try:
            conn = psycopg2.connect(
                dbname=os.getenv("DB_NAME"),
                user=os.getenv("DB_USER"),
                password=os.getenv("DB_PASSWORD"),
                host=os.getenv("DB_HOST"),
                port=os.getenv("DB_PORT"),
            )
            conn.close()
            print("‚úÖ Connexion PostgreSQL r√©ussie !", flush=True)
            return True
        except psycopg2.OperationalError as e:
            print(f"‚ö†Ô∏è PostgreSQL inaccessible : {e}", flush=True)
            time.sleep(5)

    print("‚ùå PostgreSQL inaccessible apr√®s plusieurs tentatives.", flush=True)
    return False

# Appel de la fonction pour test
if __name__ == "__main__":
    checkpostgres()


# In[102]:


# Se connecter √† la base de donn√©es
conn = psycopg2.connect(
    host=os.getenv("DB_HOST"),
    port=os.getenv("DB_PORT"),
    database=os.getenv("DB_NAME"),
    user=os.getenv("DB_USER"),
    password=os.getenv("DB_PASSWORD")

)

# Cr√©er un curseur pour faire des requ√™tes SQL
cursor = conn.cursor()

# Cr√©er les tables si elles n'existent pas d√©j√†
cursor.execute("""
CREATE TABLE IF NOT EXISTS t_users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    password_hash TEXT NOT NULL,
    date_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    ); 
""")



cursor.execute("""
    CREATE TABLE IF NOT EXISTS worldometer (
    id SERIAL PRIMARY KEY, 
    continent VARCHAR(100) NOT NULL,
    who_region VARCHAR(100) NOT NULL,
    country VARCHAR(100) NOT NULL,
    population INT NOT NULL,
    total_tests INT NOT NULL,
    total_cases INT NOT NULL,
    total_deaths INT NOT NULL,
    total_recovered INT NOT NULL,
    serious_critical INT NOT NULL
    );
""")

cursor.execute("""
    CREATE TABLE IF NOT EXISTS countries (
    id SERIAL PRIMARY KEY, 
    country VARCHAR(100) NOT NULL, 
    continent VARCHAR(100) NOT NULL,
    who_region VARCHAR(100) NOT NULL,
    population INT NOT NULL
    );
""")

cursor.execute("""
    CREATE TABLE IF NOT EXISTS health_statistics (
    id SERIAL PRIMARY KEY, 
    country VARCHAR(100) NOT NULL,
    total_cases INT NOT NULL,
    total_deaths INT NOT NULL,
    total_recovered INT NOT NULL,
    serious_critical INT NOT NULL
    );
""")

cursor.execute("""
    CREATE TABLE IF NOT EXISTS testing_statistics (
    id SERIAL PRIMARY KEY, 
    country VARCHAR(100) NOT NULL,
    total_tests INT NOT NULL
    );
""")

# Boucle pour ins√©rer les donn√©es dans la base de donn√©es
for index, row in df.iterrows():
    try:
        # Extraire les valeurs depuis chaque ligne du DataFrame

        country = row['country']
        continent = row['continent']
        who_region = row['who_region']
        population = row['population']
        total_tests = row['total_tests']
        total_cases = row['new_total_cases']  # Note que 'NewTotalCases' a √©t√© mapp√© vers 'new_total_cases'
        total_deaths = row['total_deaths']
        total_recovered = row['total_recovered']
        serious_critical = row['serious_critical']
        active_cases = row['active_cases']
        username = ['username']
        email = ['email']
        password_hash = ['password_hash']
        date_created = ['date_created']

        
        # Cr√©er les requ√™tes SQL pour ins√©rer les donn√©es dans les diff√©rentes tables

        t_users_sql = """
            INSERT INTO t_users (username, email, password_hash)
            VALUES (%s, %s, %s);
        """

        
        worldometer_sql = """
            INSERT INTO worldometer (continent, who_region, country, population, total_tests, total_cases, total_deaths, total_recovered, serious_critical)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);
        """
        
        countries_sql = """
            INSERT INTO countries (country, continent, who_region, population)
            VALUES (%s, %s, %s, %s);
        """
        
        health_statistics_sql = """
            INSERT INTO health_statistics (country, total_cases, total_deaths, total_recovered, serious_critical)
            VALUES (%s, %s, %s, %s, %s);
        """
        
        testing_statistics_sql = """
            INSERT INTO testing_statistics (country, total_tests)
            VALUES (%s, %s);
        """
        
        # Ex√©cuter les requ√™tes d'insertion avec les valeurs de la ligne
        cursor.execute(t_users_sql, (username, email, password_hash))   
        cursor.execute(countries_sql, (country, continent, who_region, population))
        cursor.execute(health_statistics_sql, (country, total_cases, total_deaths, total_recovered, serious_critical))
        cursor.execute(testing_statistics_sql, (country, total_tests))
        cursor.execute(worldometer_sql, (continent, who_region, country, population, total_tests, total_cases, total_deaths, total_recovered, serious_critical))

        # Valider les changements dans la base de donn√©es
        conn.commit()
        print(f"‚úÖ sucess")

    except psycopg2.Error as e:
        print(f"‚ùå Erreur lors de l'insertion des donn√©es pour {country}: {e}")

# Fermer le curseur et la connexion apr√®s avoir ins√©r√© toutes les donn√©es
cursor.close()
conn.close()

print("‚úÖ Donn√©es charg√©es dans PostgreSQL")



